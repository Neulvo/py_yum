# -*- coding: utf-8 -*-
"""nsmc_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GKGFh6YcbdJg3UUjsimZnJatxXoOetOI
"""

# # Mecab 설치
# !sudo apt-get install g++ openjdk-7-jdk # Install Java 1.7+
# !sudo apt-get install python-dev; pip install konlpy     # Python 2.x
# !sudo apt-get install python3-dev; pip3 install konlpy   # Python 3.x
# !sudo apt-get install curl
# !bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)

# !pip install konlpy

# !pip install Mecab

# !pip install gensim

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from tensorflow.keras.preprocessing.text import Tokenizer 
import tensorflow as tf
from tensorflow import keras 
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import Input, layers
from keras.models import load_model
from sklearn.utils import class_weight
from keras.regularizers import l2
from keras.models import load_model

from konlpy.tag import Okt
from konlpy.tag import Mecab
from gensim.models import Word2Vec

"""# 데이터 불러오기"""

train=pd.read_csv('/content/drive/My Drive/data/nsmc/ratings_train.txt',sep='\t')
test=pd.read_csv('/content/drive/My Drive/data/nsmc/ratings_test.txt',sep='\t')
full=pd.read_csv('/content/drive/My Drive/data/nsmc/ratings.txt',sep='\t')

train.dropna(inplace=True)
test.dropna(inplace=True)
full.dropna(inplace=True)

training_sentences=np.array(train['document'])
training_labels=np.array(train['label'])
testing_sentences=np.array(test['document'])
testing_labels=np.array(test['label'])
full_sentences=np.array(full['document'])
full_labels=np.array(full['label'])

"""# FastText"""

# !pip install fasttext

# import fasttext.util
# fasttext.util.download_model('ko', if_exists='ignore')
# ft = fasttext.load_model('cc.ko.300.bin')

import subprocess

with open(r"/content/drive/My Drive/data/nsmc/wiki.ko.vec", encoding='utf-8') as f:
    wiki = f.readlines()

embedding_dict = dict()

for line in wiki:
    word_vector = line.split()
    word = word_vector[0]
    word_vector_arr = np.asarray(word_vector[1:], dtype='float32') 
    embedding_dict[word] = word_vector_arr
# f.close()
print('%s개의 Embedding vector가 있습니다.' % len(embedding_dict))

"""# Base
split by (' ')

## Base_contextual

### 학습 포맷 맞추기
"""

vocab_size = 10000
max_length= 100
trunc_type = 'post'
padding_type = 'post'
oov_tok = '<OOV>'

tokenizer=Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(training_sentences)
word_index=tokenizer.word_index

def padding_maker(train_sentence, test_sentence):

  training_sequences= tokenizer.texts_to_sequences(train_sentence)
  training_padded=pad_sequences(training_sequences, maxlen=max_length,padding=padding_type, truncating=trunc_type)
  testing_sequences= tokenizer.texts_to_sequences(test_sentence)
  testing_padded=pad_sequences(testing_sequences, maxlen=max_length,padding=padding_type, truncating=trunc_type)
  
  training_padded =np.array(training_padded)
  testing_padded =np.array(testing_padded)
  return training_padded, testing_padded

training_padded, testing_padded= padding_maker(training_sentences,testing_sentences)

training_labels =np.array(training_labels)
testing_labels =np.array(testing_labels)

"""### CNN 모델"""

# Kim Yoon CNN
vocab_size = len(word_index) + 1
embedding_dim = 300
max_len = max(len(x) for x in training_padded)
class_weights = class_weight.compute_class_weight('balanced', np.unique(training_labels), training_labels)
class_weights_d = dict(enumerate(class_weights))

sequence_input = Input(shape=(max_len,), dtype='int32')

embedding_layer = tf.keras.layers.Embedding(vocab_size,
                                            embedding_dim,
                                            input_length=max_len,
                                            trainable=False)

embedded_sequences = embedding_layer(sequence_input)

convs = []
filter_sizes = [3,4,5]

for fsz in filter_sizes:
    x = layers.Conv1D(128, fsz, activation='relu',padding='same',kernel_regularizer=l2(1e-3))(embedded_sequences)
    x = layers.MaxPooling1D()(x)
    convs.append(x)
    
x = layers.Concatenate(axis=-1)(convs)
x = layers.Flatten()(x)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)
output = layers.Dense(1, activation='sigmoid')(x)

"""### 컴파일 및 피팅"""

model_base = keras.models.Model(sequence_input, output)
model_base.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

model_base.summary()

callback=tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0005, patience=1, verbose=0, mode='auto')
# Fit model
history_base = model_base.fit(training_padded, training_labels,
                    class_weight = class_weights_d,
                    epochs=10,
                    callbacks=[callback],
                    verbose=True,
                    validation_data=(testing_padded, testing_labels),
                    batch_size=50)
loss, accuracy = model_base.evaluate(training_padded, training_labels, verbose=True)
print("Training Accuracy: {:.4f}".format(accuracy))
loss, accuracy = model_base.evaluate(testing_padded, testing_labels, verbose=False)
print("Testing Accuracy:  {:.4f}".format(accuracy))

"""### ACC, LOSS 그래프"""

def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()

plot_graphs(history_base, 'accuracy')
plot_graphs(history_base, 'loss')

"""### 예측"""

sentence = ['영화 너무 재밌더라.', '영화 개노잼. 다신 안 봐.']
sequence = tokenizer.texts_to_sequences(sentence)
padded = pad_sequences(sequence, maxlen=max_length, padding=padding_type, truncating=trunc_type)
print(model_base.predict(padded).round(2))

"""## Base_FastText"""

# 임베딩 메트릭스
embedding_matrix = np.zeros((vocab_size , embedding_dim))
# 단어 집합 크기의 행과 100개의 열을 가지는 행렬 생성. 값은 전부 0으로 채워진다.
np.shape(embedding_matrix)
out_index = 0
for word, i in word_index.items(): # 훈련 데이터의 단어 집합에서 단어를 1개씩 꺼내온다.
    embedding_vector = embedding_dict.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector
    else:
        out_index += 1

print(out_index)

"""### CNN 모델"""

# Kim Yoon CNN
vocab_size = len(word_index) + 1
embedding_dim = 300
max_len = max(len(x) for x in training_padded)
class_weights = class_weight.compute_class_weight('balanced', np.unique(training_labels), training_labels)
class_weights_d = dict(enumerate(class_weights))

sequence_input = Input(shape=(max_len,), dtype='int32')

embedding_layer = tf.keras.layers.Embedding(vocab_size,
                                            embedding_dim,
                                            weights=[embedding_matrix],
                                            input_length=max_len,
                                            trainable=False)

embedded_sequences = embedding_layer(sequence_input)

convs = []
filter_sizes = [3,4,5]

for fsz in filter_sizes:
    x = layers.Conv1D(128, fsz, activation='relu',padding='same',kernel_regularizer=l2(1e-3))(embedded_sequences)
    x = layers.MaxPooling1D()(x)
    convs.append(x)
    
x = layers.Concatenate(axis=-1)(convs)
x = layers.Flatten()(x)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)
output = layers.Dense(1, activation='sigmoid')(x)

"""### 컴파일 및 피팅"""

model_base_ff = keras.models.Model(sequence_input, output)
model_base_ff.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

model_base_ff.summary()

callback=tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0005, patience=1, verbose=0, mode='auto')
# Fit model
history_base_ff = model_base_ff.fit(training_padded, training_labels,
                    class_weight = class_weights_d,
                    epochs=10,
                    callbacks=[callback],
                    verbose=True,
                    validation_data=(testing_padded, testing_labels),
                    batch_size=50)
loss, accuracy = model_base_ff.evaluate(training_padded, training_labels, verbose=True)
print("Training Accuracy: {:.4f}".format(accuracy))
loss, accuracy = model_base_ff.evaluate(testing_padded, testing_labels, verbose=False)
print("Testing Accuracy:  {:.4f}".format(accuracy))

"""### ACC, LOSS 그래프"""

plot_graphs(history_base_ff, 'accuracy')
plot_graphs(history_base_ff, 'loss')

"""### 예측"""

sentence = ['영화 너무 재밌더라.', '영화 개노잼. 다신 안 봐.']
sequence = tokenizer.texts_to_sequences(sentence)
padded = pad_sequences(sequence, maxlen=max_length, padding=padding_type, truncating=trunc_type)
print(model_base_ff.predict(padded).round(2))

"""## Base_Word2Vec"""

token_list=[tokens.split() for tokens in full_sentences.tolist()]

word2vec = Word2Vec(
    sentences = token_list,
    size = embedding_dim,
    alpha = 0.025,
    min_count = 1,
    window=8,
    sample = 0.001,
    sg = 1,
    iter = 10
)

# 임베딩 메트릭스
embedding_matrix = np.zeros((vocab_size , embedding_dim))
# 단어 집합 크기의 행과 100개의 열을 가지는 행렬 생성. 값은 전부 0으로 채워진다.
np.shape(embedding_matrix)
out_index = 0
for word, i in word_index.items(): # 훈련 데이터의 단어 집합에서 단어를 1개씩 꺼내온다.
  try:
    embedding_vector = word2vec.wv.__getitem__(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector
    else:
        out_index += 1
  except:
    out_index+=1

print(out_index)

"""### CNN 모델"""

# Kim Yoon CNN
vocab_size = len(word_index) + 1
embedding_dim = 300
max_len = max(len(x) for x in training_padded)
class_weights = class_weight.compute_class_weight('balanced', np.unique(training_labels), training_labels)
class_weights_d = dict(enumerate(class_weights))

sequence_input = Input(shape=(max_len,), dtype='int32')

embedding_layer = tf.keras.layers.Embedding(vocab_size,
                                            embedding_dim,
                                            weights=[embedding_matrix],
                                            input_length=max_len,
                                            trainable=False)

embedded_sequences = embedding_layer(sequence_input)

convs = []
filter_sizes = [3,4,5]

for fsz in filter_sizes:
    x = layers.Conv1D(128, fsz, activation='relu',padding='same',kernel_regularizer=l2(1e-3))(embedded_sequences)
    x = layers.MaxPooling1D()(x)
    convs.append(x)
    
x = layers.Concatenate(axis=-1)(convs)
x = layers.Flatten()(x)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)
output = layers.Dense(1, activation='sigmoid')(x)

"""### 컴파일 및 피팅"""

model_base_ff = keras.models.Model(sequence_input, output)
model_base_ff.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

model_base_ff.summary()

callback=tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0005, patience=1, verbose=0, mode='auto')
# Fit model
history_base_ff = model_base_ff.fit(training_padded, training_labels,
                    class_weight = class_weights_d,
                    epochs=10,
                    callbacks=[callback],
                    verbose=True,
                    validation_data=(testing_padded, testing_labels),
                    batch_size=50)
loss, accuracy = model_base_ff.evaluate(training_padded, training_labels, verbose=True)
print("Training Accuracy: {:.4f}".format(accuracy))
loss, accuracy = model_base_ff.evaluate(testing_padded, testing_labels, verbose=False)
print("Testing Accuracy:  {:.4f}".format(accuracy))

"""### ACC, LOSS 그래프"""

plot_graphs(history_base_ff, 'accuracy')
plot_graphs(history_base_ff, 'loss')

"""### 예측"""

sentence = ['영화 너무 재밌더라.', '영화 개노잼. 다신 안 봐.']
sequence = tokenizer.texts_to_sequences(sentence)
padded = pad_sequences(sequence, maxlen=max_length, padding=padding_type, truncating=trunc_type)
print(model_base_ff.predict(padded).round(2))

"""# OKT
tokenized by okt

## OKT_contextual

### 학습 포맷 맞추기
"""

okt=Okt()

def okt_nizer(base_sentence):
  okt_sentence=[]
  for sentence in base_sentence:
    pos_tag=okt.pos(sentence)
    tok_sentence=[]
    for token, pos in pos_tag:
      if pos[0] in ['N','V']:
        tok_sentence.append(token)
    okt_sentence.append(' '.join(tok_sentence))
  okt_sentence=np.array(okt_sentence)
  return okt_sentence

okt_training=okt_nizer(training_sentences)
okt_testing=okt_nizer(testing_sentences)

tokenizer=Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(okt_training)
okt_word_index=tokenizer.word_index

okt_training_padded, okt_testing_padded = padding_maker(okt_training,okt_testing)

"""### CNN 모델"""

# Kim Yoon CNN
vocab_size = len(word_index) +1
embedding_dim = 300
max_len = max(len(x) for x in okt_training_padded)
class_weights = class_weight.compute_class_weight('balanced', np.unique(training_labels), training_labels)
class_weights_d = dict(enumerate(class_weights))

sequence_input = Input(shape=(max_len,), dtype='int32')

embedding_layer = tf.keras.layers.Embedding(vocab_size,
                                            embedding_dim,
                                            input_length=max_len,
                                            trainable=False)

embedded_sequences = embedding_layer(sequence_input)

convs = []
filter_sizes = [3,4,5]

for fsz in filter_sizes:
    x = layers.Conv1D(128, fsz, activation='relu',padding='same',kernel_regularizer=l2(1e-3))(embedded_sequences)
    x = layers.MaxPooling1D()(x)
    convs.append(x)
    
x = layers.Concatenate(axis=-1)(convs)
x = layers.Flatten()(x)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)
output = layers.Dense(1, activation='sigmoid')(x)

"""### 컴파일 및 피팅"""

model_okt_base = keras.models.Model(sequence_input, output)
model_okt_base.compile(loss='binary_crossentropy',
                       optimizer='adam',
                       metrics=['accuracy'])

model_okt_base.summary()

callback=tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0005, patience=1, verbose=0, mode='auto')
# Fit model
history_okt_base = model_okt_base.fit(okt_training_padded, training_labels,
                    class_weight = class_weights_d,
                    epochs=10,
                    callbacks=[callback],
                    verbose=True,
                    validation_data=(okt_testing_padded, testing_labels),
                    batch_size=50)
loss, accuracy = model_okt_base.evaluate(okt_training_padded, training_labels, verbose=True)
print("Training Accuracy: {:.4f}".format(accuracy))
loss, accuracy = model_okt_base.evaluate(okt_testing_padded, testing_labels, verbose=False)
print("Testing Accuracy:  {:.4f}".format(accuracy))

"""### ACC, LOSS 그래프"""

plot_graphs(history_okt_base, 'accuracy')
plot_graphs(history_okt_base, 'loss')

"""### 예측"""

sentence = ['영화 너무 재밌더라.', '영화 개노잼. 다신 안 봐.']
sequence = tokenizer.texts_to_sequences(sentence)
padded = pad_sequences(sequence, maxlen=max_length, padding=padding_type, truncating=trunc_type)
print(model_okt_base.predict(padded).round(2))

"""## OKT_FastText"""

# 임베딩 메트릭스
embedding_matrix = np.zeros((vocab_size , embedding_dim))
# 단어 집합 크기의 행과 100개의 열을 가지는 행렬 생성. 값은 전부 0으로 채워진다.
np.shape(embedding_matrix)
out_index = 0
for word, i in okt_word_index.items(): # 훈련 데이터의 단어 집합에서 단어를 1개씩 꺼내온다.
    embedding_vector = embedding_dict.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector
    else:
        out_index += 1

print(out_index)

"""### CNN 모델"""

# Kim Yoon CNN
vocab_size = len(word_index) + 1
embedding_dim = 300
max_len = max(len(x) for x in okt_training_padded)
class_weights = class_weight.compute_class_weight('balanced', np.unique(training_labels), training_labels)
class_weights_d = dict(enumerate(class_weights))

sequence_input = Input(shape=(max_len,), dtype='int32')

embedding_layer = tf.keras.layers.Embedding(vocab_size,
                                            embedding_dim,
                                            weights=[embedding_matrix],
                                            input_length=max_len,
                                            trainable=False)

embedded_sequences = embedding_layer(sequence_input)

convs = []
filter_sizes = [3,4,5]

for fsz in filter_sizes:
    x = layers.Conv1D(128, fsz, activation='relu',padding='same',kernel_regularizer=l2(1e-3))(embedded_sequences)
    x = layers.MaxPooling1D()(x)
    convs.append(x)
    
x = layers.Concatenate(axis=-1)(convs)
x = layers.Flatten()(x)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)
output = layers.Dense(1, activation='sigmoid')(x)

"""### 컴파일 및 피팅"""

model_okt_ff = keras.models.Model(sequence_input, output)
model_okt_ff.compile(loss='binary_crossentropy',
                       optimizer='adam',
                       metrics=['accuracy'])

model_okt_ff.summary()

callback=tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0005, patience=1, verbose=0, mode='auto')
# Fit model
history_okt_ff = model_okt_ff.fit(okt_training_padded, training_labels,
                    class_weight = class_weights_d,
                    epochs=10,
                    callbacks=[callback],
                    verbose=True,
                    validation_data=(okt_testing_padded, testing_labels),
                    batch_size=50)
loss, accuracy = model_okt_ff.evaluate(okt_training_padded, training_labels, verbose=True)
print("Training Accuracy: {:.4f}".format(accuracy))
loss, accuracy = model_okt_ff.evaluate(okt_testing_padded, testing_labels, verbose=False)
print("Testing Accuracy:  {:.4f}".format(accuracy))

"""### ACC, LOSS 그래프"""

plot_graphs(history_okt_ff, 'accuracy')
plot_graphs(history_okt_ff, 'loss')

"""### 예측"""

sentence = ['영화 너무 재밌더라.', '영화 개노잼. 다신 안 봐.']
sequence = tokenizer.texts_to_sequences(sentence)
padded = pad_sequences(sequence, maxlen=max_length, padding=padding_type, truncating=trunc_type)
print(model_okt_ff.predict(padded).round(2))

"""## OKT_Word2Vec"""

token_list=[]
for sentence in full_sentences:
  pos_tag=okt.pos(sentence)
  tok_sentence=[]
  for token, pos in pos_tag:
    if pos[0] in ['N','V']:
      tok_sentence.append(token)
  token_list.append(tok_sentence)

word2vec = Word2Vec(
    sentences = token_list,
    size = embedding_dim,
    alpha = 0.025,
    min_count = 1,
    window=8,
    sample = 0.001,
    sg = 1,
    iter = 10
)

# 임베딩 메트릭스
embedding_matrix = np.zeros((vocab_size , embedding_dim))
# 단어 집합 크기의 행과 100개의 열을 가지는 행렬 생성. 값은 전부 0으로 채워진다.
np.shape(embedding_matrix)
out_index = 0
for word, i in word_index.items(): # 훈련 데이터의 단어 집합에서 단어를 1개씩 꺼내온다.
  try:
    embedding_vector = word2vec.wv.__getitem__(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector
    else:
        out_index += 1
  except:
    out_index+=1

print(out_index)

"""### CNN 모델"""

# Kim Yoon CNN
vocab_size = len(word_index) + 1
embedding_dim = 300
max_len = max(len(x) for x in training_padded)
class_weights = class_weight.compute_class_weight('balanced', np.unique(training_labels), training_labels)
class_weights_d = dict(enumerate(class_weights))

sequence_input = Input(shape=(max_len,), dtype='int32')

embedding_layer = tf.keras.layers.Embedding(vocab_size,
                                            embedding_dim,
                                            weights=[embedding_matrix],
                                            input_length=max_len,
                                            trainable=False)

embedded_sequences = embedding_layer(sequence_input)

convs = []
filter_sizes = [3,4,5]

for fsz in filter_sizes:
    x = layers.Conv1D(128, fsz, activation='relu',padding='same',kernel_regularizer=l2(1e-3))(embedded_sequences)
    x = layers.MaxPooling1D()(x)
    convs.append(x)
    
x = layers.Concatenate(axis=-1)(convs)
x = layers.Flatten()(x)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)
output = layers.Dense(1, activation='sigmoid')(x)

"""### 컴파일 및 피팅"""

model_okt_w2 = keras.models.Model(sequence_input, output)
model_okt_w2.compile(loss='binary_crossentropy',
                       optimizer='adam',
                       metrics=['accuracy'])

model_okt_w2.summary()

callback=tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0005, patience=1, verbose=0, mode='auto')
# Fit model
history_okt_w2 = model_okt_w2.fit(okt_training_padded, training_labels,
                    class_weight = class_weights_d,
                    epochs=10,
                    callbacks=[callback],
                    verbose=True,
                    validation_data=(okt_testing_padded, testing_labels),
                    batch_size=50)
loss, accuracy = model_okt_w2.evaluate(okt_training_padded, training_labels, verbose=True)
print("Training Accuracy: {:.4f}".format(accuracy))
loss, accuracy = model_okt_w2.evaluate(okt_testing_padded, testing_labels, verbose=False)
print("Testing Accuracy:  {:.4f}".format(accuracy))

"""### ACC, LOSS 그래프"""

plot_graphs(history_okt_w2, 'accuracy')
plot_graphs(history_okt_w2, 'loss')

"""### 예측"""

sentence = ['영화 너무 재밌더라.', '영화 개노잼. 다신 안 봐.']
sequence = tokenizer.texts_to_sequences(sentence)
padded = pad_sequences(sequence, maxlen=max_length, padding=padding_type, truncating=trunc_type)
print(model_okt_w2.predict(padded).round(2))

"""# Mecab
tokenized by mecab

## Mecab_contextual

### 학습 포맷 맞추기
"""

mecab=Mecab()

def mecab_nizer(base_sentence):
  mecab_sentence=[]
  for sentence in base_sentence:
    pos_tag=mecab.pos(sentence)
    tok_sentence=[]
    for token, pos in pos_tag:
      if pos[0] in ['N','V']:
        tok_sentence.append(token)
    mecab_sentence.append(' '.join(tok_sentence))
  mecab_sentence=np.array(mecab_sentence)
  return mecab_sentence

mecab_training=mecab_nizer(training_sentences)
mecab_testing=mecab_nizer(testing_sentences)

tokenizer=Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(mecab_training)
mecab_word_index=tokenizer.word_index

mecab_training_padded, mecab_testing_padded = padding_maker(mecab_training,mecab_testing)

vocab_size = len(mecab_word_index) + 1
max_len = max(len(x) for x in mecab_training_padded)
embedding_dim = 300
vocab_size = len(mecab_word_index) + 1

"""### CNN 모델"""

# Kim Yoon CNN
vocab_size = len(word_index) + 1
embedding_dim = 300
max_len = max(len(x) for x in mecab_training_padded)
class_weights = class_weight.compute_class_weight('balanced', np.unique(training_labels), training_labels)
class_weights_d = dict(enumerate(class_weights))

sequence_input = Input(shape=(max_len,), dtype='int32')

embedding_layer = tf.keras.layers.Embedding(vocab_size,
                                            embedding_dim,
                                            input_length=max_len,
                                            trainable=False)

embedded_sequences = embedding_layer(sequence_input)

convs = []
filter_sizes = [3,4,5]

for fsz in filter_sizes:
    x = layers.Conv1D(128, fsz, activation='relu',padding='same',kernel_regularizer=l2(1e-3))(embedded_sequences)
    x = layers.MaxPooling1D()(x)
    convs.append(x)
    
x = layers.Concatenate(axis=-1)(convs)
x = layers.Flatten()(x)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)
output = layers.Dense(1, activation='sigmoid')(x)

"""### 컴파일 및 피팅"""

model_mecab_base = keras.models.Model(sequence_input, output)
model_mecab_base.compile(loss='binary_crossentropy',
                       optimizer='adam',
                       metrics=['accuracy'])

model_mecab_base.summary()

callback=tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0005, patience=1, verbose=0, mode='auto')
# Fit model
history_mecab_base = model_mecab_base.fit(mecab_training_padded, training_labels,
                    class_weight = class_weights_d,
                    epochs=10,
                    callbacks=[callback],
                    verbose=True,
                    validation_data=(mecab_testing_padded, testing_labels),
                    batch_size=50)
loss, accuracy = model_mecab_base.evaluate(mecab_training_padded, training_labels, verbose=True)
print("Training Accuracy: {:.4f}".format(accuracy))
loss, accuracy = model_mecab_base.evaluate(mecab_testing_padded, testing_labels, verbose=False)
print("Testing Accuracy:  {:.4f}".format(accuracy))

"""### ACC, LOSS 그래프"""

plot_graphs(history_mecab_base, 'accuracy')
plot_graphs(history_mecab_base, 'loss')

"""### 예측"""

sentence = ['영화 너무 재밌더라.', '영화 개노잼. 다신 안 봐.']
sequence = tokenizer.texts_to_sequences(sentence)
padded = pad_sequences(sequence, maxlen=max_length, padding=padding_type, truncating=trunc_type)
print(model_mecab_base.predict(padded).round(2))

"""## Mecab_FastText"""

# 임베딩 메트릭스
embedding_matrix = np.zeros((vocab_size , embedding_dim))
# 단어 집합 크기의 행과 100개의 열을 가지는 행렬 생성. 값은 전부 0으로 채워진다.
np.shape(embedding_matrix)
out_index = 0
for word, i in mecab_word_index.items(): # 훈련 데이터의 단어 집합에서 단어를 1개씩 꺼내온다.
    embedding_vector = embedding_dict.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector
    else:
        out_index += 1

print(out_index)

"""### CNN 모델"""

# Kim Yoon CNN
vocab_size = len(word_index) + 1
embedding_dim = 300
max_len = max(len(x) for x in mecab_training_padded)
class_weights = class_weight.compute_class_weight('balanced', np.unique(training_labels), training_labels)
class_weights_d = dict(enumerate(class_weights))

sequence_input = Input(shape=(max_len,), dtype='int32')

embedding_layer = tf.keras.layers.Embedding(vocab_size,
                                            embedding_dim,
                                            weights=[embedding_matrix],
                                            input_length=max_len,
                                            trainable=False)

embedded_sequences = embedding_layer(sequence_input)

convs = []
filter_sizes = [3,4,5]

for fsz in filter_sizes:
    x = layers.Conv1D(128, fsz, activation='relu',padding='same',kernel_regularizer=l2(1e-3))(embedded_sequences)
    x = layers.MaxPooling1D()(x)
    convs.append(x)
    
x = layers.Concatenate(axis=-1)(convs)
x = layers.Flatten()(x)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)
output = layers.Dense(1, activation='sigmoid')(x)

"""### 컴파일 및 피팅"""

model_mecab_ff = keras.models.Model(sequence_input, output)
model_mecab_ff.compile(loss='binary_crossentropy',
                       optimizer='adam',
                       metrics=['accuracy'])

model_mecab_ff.summary()

callback=tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0005, patience=1, verbose=0, mode='auto')
# Fit model
history_mecab_ff = model_mecab_ff.fit(mecab_training_padded, training_labels,
                    class_weight = class_weights_d,
                    epochs=10,
                    callbacks=[callback],
                    verbose=True,
                    validation_data=(mecab_testing_padded, testing_labels),
                    batch_size=50)
loss, accuracy = model_mecab_ff.evaluate(mecab_training_padded, training_labels, verbose=True)
print("Training Accuracy: {:.4f}".format(accuracy))
loss, accuracy = model_mecab_ff.evaluate(mecab_testing_padded, testing_labels, verbose=False)
print("Testing Accuracy:  {:.4f}".format(accuracy))

"""### ACC, LOSS 그래프"""

plot_graphs(history_mecab_ff, 'accuracy')
plot_graphs(history_mecab_ff, 'loss')

"""### 예측"""

sentence = ['영화 너무 재밌더라.', '영화 개노잼. 다신 안 봐.']
sequence = tokenizer.texts_to_sequences(sentence)
padded = pad_sequences(sequence, maxlen=max_length, padding=padding_type, truncating=trunc_type)
print(model_mecab_ff.predict(padded).round(2))

"""## Mecab_Word2Vec"""

token_list=[]
for sentence in full_sentences:
  pos_tag=mecab.pos(sentence)
  tok_sentence=[]
  for token, pos in pos_tag:
    if pos[0] in ['N','V']:
      tok_sentence.append(token)
  token_list.append(tok_sentence)

word2vec = Word2Vec(
    sentences = token_list,
    size = embedding_dim,
    alpha = 0.025,
    min_count = 1,
    window=8,
    sample = 0.001,
    sg = 1,
    iter = 10
)

# 임베딩 메트릭스
embedding_matrix = np.zeros((vocab_size , embedding_dim))
# 단어 집합 크기의 행과 100개의 열을 가지는 행렬 생성. 값은 전부 0으로 채워진다.
np.shape(embedding_matrix)
out_index = 0
for word, i in word_index.items(): # 훈련 데이터의 단어 집합에서 단어를 1개씩 꺼내온다.
  try:
    embedding_vector = word2vec.wv.__getitem__(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector
    else:
        out_index += 1
  except:
    out_index +=1

print(out_index)

"""### CNN 모델"""

# Kim Yoon CNN
vocab_size = len(word_index) + 1
embedding_dim = 300
max_len = max(len(x) for x in training_padded)
class_weights = class_weight.compute_class_weight('balanced', np.unique(training_labels), training_labels)
class_weights_d = dict(enumerate(class_weights))

sequence_input = Input(shape=(max_len,), dtype='int32')

embedding_layer = tf.keras.layers.Embedding(vocab_size,
                                            embedding_dim,
                                            weights=[embedding_matrix],
                                            input_length=max_len,
                                            trainable=False)

embedded_sequences = embedding_layer(sequence_input)

convs = []
filter_sizes = [3,4,5]

for fsz in filter_sizes:
    x = layers.Conv1D(128, fsz, activation='relu',padding='same',kernel_regularizer=l2(1e-3))(embedded_sequences)
    x = layers.MaxPooling1D()(x)
    convs.append(x)
    
x = layers.Concatenate(axis=-1)(convs)
x = layers.Flatten()(x)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)
output = layers.Dense(1, activation='sigmoid')(x)

"""### 컴파일 및 피팅"""

model_mecab_w2 = keras.models.Model(sequence_input, output)
model_mecab_w2.compile(loss='binary_crossentropy',
                       optimizer='adam',
                       metrics=['accuracy'])

model_mecab_w2.summary()

callback=tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0005, patience=1, verbose=0, mode='auto')
# Fit model
history_mecab_w2 = model_mecab_w2.fit(mecab_training_padded, training_labels,
                    class_weight = class_weights_d,
                    epochs=10,
                    callbacks=[callback],
                    verbose=True,
                    validation_data=(mecab_testing_padded, testing_labels),
                    batch_size=50)
loss, accuracy = model_mecab_w2.evaluate(mecab_training_padded, training_labels, verbose=True)
print("Training Accuracy: {:.4f}".format(accuracy))
loss, accuracy = model_mecab_w2.evaluate(mecab_testing_padded, testing_labels, verbose=False)
print("Testing Accuracy:  {:.4f}".format(accuracy))

"""### ACC, LOSS 그래프"""

plot_graphs(history_mecab_w2, 'accuracy')
plot_graphs(history_mecab_w2, 'loss')

"""### 예측"""

sentence = ['영화 너무 재밌더라.', '영화 개노잼. 다신 안 봐.']
sequence = tokenizer.texts_to_sequences(sentence)
padded = pad_sequences(sequence, maxlen=max_length, padding=padding_type, truncating=trunc_type)
print(model_mecab_w2.predict(padded).round(2))

